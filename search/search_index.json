{"config": {"lang": ["en"], "separator": "[\\s\\-]+", "pipeline": ["stopWordFilter"]}, "docs": [{"location": "", "title": "Welcome to the Environmental Change and Human Outcomes (ECHO) Lab!", "text": ""}, {"location": "#what_is_the_echolab", "title": "What is the ECHOLab?", "text": "<p>The EchoLab, now part of the Doerr School of Sustainability, is a multidisciplinary team spanning from social and earth scientists, and epidemiologists with the common goal of measuring and understanding the effects of environmental change on present and future human systems. </p>"}, {"location": "#what_is_in_here", "title": "What is in here?", "text": "<p>This is a compilation of different advice from past and current members of the team. We aspire to have a live-document that unifies the knowledge available in disparate places across the web, such as computation resources, graduate student resources or RAs benefits and opportunities. We also intent to have an on-boarding document that soft-lands the new members of our team. </p> <p>This document is crowd-sourced and it needs your help to keep it up-to-date and in good shape for others to use. Once you become a tenured ECHOer, you need to share your new tricks here to others. See the contributing guidelines.</p>"}, {"location": "computing/", "title": "Computing", "text": ""}, {"location": "computing/#computation_at_stanford", "title": "Computation at Stanford", "text": "<p>Stanford has several places for computation! All students (undergrad and grad) have access to Farmshare, a small cluster with a queue and with very limited resources. Additionally, our group has access to Sherlock, the main computing cluster and a very cool and powerful tool when used responsibly. </p> <p>We do not own any nodes at Sherlock, that means that we have to use a queue system and stay in line until computing resources are free to run our jobs. Sherlock's line is shared with all the university users, but as part of the School of Sustainability we have one of the largest partitions in Sherlock that is only used by researchers within the school. This allows our jobs to stay less time in the queue as the number of users is just a subset of all of Sherlock's users. </p>"}, {"location": "computing/#what_is_a_cluster", "title": "What is a cluster?", "text": "<p>A computer cluster is in rough terms a set of shared computers that are connected to each other while sharing memory and processing power. Thus, we can run jobs a (jobs is the name we give to running a script on the cluster) across nodes or within each of the nodes (nodes are BIG computers). All nodes are coordinated by a scheduler, in Sherlock case, we use SLURM, a very flexible scheduler used by most of the big clusters around the world, including the beautiful Cheyenne, the NCAR super-computer. </p> <p>Sherlock is a shared resource</p> <p>Every time you use Sherlock, SLURM will penalize you and the whole lab! SLURM relies on the FairShare, an algorithm that decides what's the fair use of computation resources for each user and group. Therefore, everytime you run a job be sure of asking the resources you really need (i.e. number of hours, cores, and memory).</p> <p>In general, clusters have three types of nodes: </p> <ul> <li>Login: Nodes meant to welcome the user and meant for anything but    computation. They're named usually: <code>sh02-lnx</code>, where <code>x</code> is the login node    identity number. </li> <li>Computing: This nodes will run jobs. Any code always runs in these    nodes. These nodes are named usually: <code>sh03-09n59</code>, where <code>09n59</code> is    a specific node in the whole cluster. </li> <li>Storage: Rarely used, but you can access to nodes that are meant just    for data transmission. This can be either moving a lot of data out or to    Sherlock. </li> </ul> <p>In this document we will focus on the SERC partition. You can check the partition characteristics by using the <code>sh_part</code> command. If you have access to other partitions or your PI (in case you have a second PI) has nodes, you will see the information for those nodes there too. </p>"}, {"location": "computing/#modules", "title": "Modules", "text": "<p>Having all the software binaries loaded into the <code>$PATH</code>1 can lead to chaos. For this reason, Sherlock uses the Modules system. Loading modules is part of the life in the cluster and is essential not only for running code, but also to compile packages (i.e. installing R's <code>sf</code> needs you to load several modules. Same is true for many Python libraries). </p> <p>Sherlock has some amazing documentation on Modules, so we won't cover that here. Just some basics for our lab:  </p> PythonROther stuff <pre><code>module load python/3.9.0\n</code></pre> <pre><code>module load R/4.2.0\n</code></pre> <pre><code>module spider &lt;other stuff to look for&gt;\n</code></pre>"}, {"location": "computing/#storage", "title": "Storage", "text": "<p>We have plenty of storage space in Sherlock! For job running, is good to use fast file systems, like the <code>$SCRATCH</code> space. For permanent data storage and files that will be concurrently used, the <code>$OAK</code> and <code>$GROUP_HOME</code> are good alternatives. As all good things in the cluster, remember that <code>$OAK</code> is a shared space within the SDSS, so think of other users before wanting to store those 100 Tb of CMIP data you probably do not need. </p> Storage Path Total space Time Limit <code>$OAK</code> <code>/oak/stanford/schools/ees/mburke</code> 1 Pb [100 Tb/group] \\(\\infty\\) <code>$GROUP_HOME</code> <code>/home/groups/mburke</code> 100 Tb \\(\\infty\\) <code>$GROUP_SCRATCH</code> <code>/scratch/groups/mburke</code> 100 Tb 90 days <p><code>SCRATCH</code> is deleted every 90 days!</p> <p>Be careful with the scratch space! Be sure to remove all the files you need after you run a job. Files in this storage are not backed-up, so there will be no way to recover.</p>"}, {"location": "computing/#oak", "title": "Oak", "text": "<p>Oak is shared with all the SDSS. You can create folders in the root of the server (the path in the table above) if you need to share files with people across the school (i.e. ESS, EPS, or rarely, E-IPER). For lab-wide data, we use the <code>mburke</code> directory. We do not have any particular nomenclature for our filesystem, but try to be use sensible and informative names: </p> <ul> <li>\ud83d\udeab <code>$OAK/mburke/results</code></li> <li>\u2705 <code>$OAK/mburke/&lt;project_name&gt;</code></li> <li>\u2705 <code>$OAK/mburke/ERA_5_1979_2023</code></li> </ul> <p>To avoid permission and possible file corruption avoid cloning repos directly into <code>$OAK</code>, use your <code>$HOME</code> for that and store your data in any of the storages described above</p>"}, {"location": "computing/#running_jobs", "title": "Running jobs", "text": "<p>There are different ways of running jobs in Sherlock. Compared with other clusters, Sherlock does not have any computing quotas, rather you can run as many jobs you want until the scheduler  stop giving giving you jobs. In general, we have two ways of running jobs: </p>"}, {"location": "computing/#in-line", "title": "In-line", "text": "<p>SLURM allow us to request nodes and run jobs directly into them! We have several use cases for this request: </p> <ul> <li>Testing a pipeline: I have built a Python script <code>hello.py</code> and want to test    it before submitting a larger job.</li> <li>Running small tests: I have a GPU job and I want to test my Python    environment is correctly compiling the right CUDA version. I cannot test    this is a login node. </li> <li>Running a Jupyter Notebook: I want to run a quick jupyter notebook without    having to deal with a job submission.</li> </ul> <p>In-line jobs are very useful, especially for testing and debugging, but remember that is always better to have pipelines that are less interactive and that can run unsupervised. To run in-line jobs, you have to use the <code>sdev</code> command: </p> <p>Example</p> <p><code>sdev</code> commands:     <pre><code>$ sdev -h\nsdev: start an interactive shell on a compute node.\n\nUsage: sdev [OPTIONS]\nOptional arguments:\n        -c      number of CPU cores to request (OpenMP/pthreads, default: 1)\n-g      number of GPUs to request (default: none)\n-n      number of tasks to request (MPI ranks, default: 1)\n-N      number of nodes to request (default: 1)\n-m      memory amount to request (default: 4GB)\n-p      partition to run the job in (default: dev)\n-t      time limit (default: 01:00:00)\n-r      allocate resources from the named reservation (default: none)\n-J      job name (default: sdev)\n-q      quality of service to request for the job (default: normal)\nNote: the default partition only allows for limited amount of resources.\n    If you need more, your job will be rejected unless you specify an\n    alternative partition with -p.\n</code></pre></p> <p>You can request a job with 1 node, 16 cores and 32 Gb of RAM in the SERC partition (just like the average MacBookPro) for 30 minutes using the following:</p> <pre><code>sdev -n 1 -c 16 -m 16G -p serc -t 00:30:00 --pty bash\n</code></pre> <p>Once you run this job, you will get assigned a job number and when the resources are allocated you will have access to that computing node terminal. Inside the computing node you have access to any folder or file stored in Sherlock and you can run any binary using the <code>module</code> command. </p>"}, {"location": "computing/#batch_job", "title": "Batch job", "text": "<p>Compared to the in-line job, we won't have any access to the computing node terminal. The job will run completely unsupervised and if it fails then the job will stop executing and will be cancelled and two files will be written: an output file (usually with an <code>.out</code> extension), and an error file with all the error contents (with an <code>.err</code> extension)</p> Job exampleAnother exampleSubmit your job <pre><code>#!/usr/bin/bash\n#SBATCH --job-name=test_job\n#SBATCH --output=test_job.out\n#SBATCH --error=test_job.err\n#SBATCH --time=00:30:00\n#SBATCH -p serc\n#SBATCH --nodes 1\n#SBATCH --mem 16GB\n#SBATCH -c 4\n#SBATCH --mail-user=ihigueme@stanford.edu\n#SBATCH --mail-type=ALL\nsleep 10\necho \"Hello from the ${HOST}\"\n</code></pre> <pre><code>#!/usr/bin/bash\n#SBATCH --job-name=test_job\n#SBATCH --output=test_job.out\n#SBATCH --error=test_job.err\n#SBATCH --time=00:30:00\n#SBATCH -p gpu\n#SBATCH --nodes 1\n#SBATCH --mem 16GB\n#SBATCH --gpu 1\n#SBATCH -c 4\n#SBATCH --mail-user=ihigueme@stanford.edu\n#SBATCH --mail-type=ALL\nsleep 10\necho \"Hello from the ${HOST} with a GPU\"\n</code></pre> <pre><code>$ sbatch test_job.sbatch\nSubmitted batch job 22312518 # This is your job id\n</code></pre> <p>As you can see, there is some overlap between the options we used in <code>sdev</code> and the ones we use in our batch job file. Any SLURM command should be prepended by the <code>#SBATCH</code> flag, this is telling the computer that is a command for the scheduler and not a local variable. Let's go over some of the commands use above: </p> <code>--job-name</code> Name for the job. Is useful for traceability when calling <code>squeue</code> or just to check the history of commands ran in the past. <code>--output</code> and <code>--error</code> Name for output and error files. Usually saved in the same directory where the batch file is located. You can specify another path. If left empty, the default will be <code>&lt;job-name&gt;&lt;job-id&gt;.err</code> <code>--time</code> Time we are asking the scheduler to keep the job alive. Once the time is done, the job will be stopped. Plan accordignly and test before running a long job. Sherlock has limits, so jobs shouldn't last more than 7 days. <code>-p</code> Paritition. The default is using the main queue, but there are many available (list them using the <code>sh_part</code> command). Use <code>dev</code> if you want to test, and <code>serc</code> when you're ready to send jobs. <code>--nodes</code> Number of nodes to request. Usually this is set to 1 because it fits most of    the vanilla parallelization schemes in Python and R. Use more than 1 node    if, and only if, you're using OpenMPI powered code (i.e. C++ processes and    sometimes Python's Dask). If using more than one node, many new options are    open: <code>--ntasks-per-node</code>, and <code>--cpus-per-task</code>. Read more on the SLURM    documentation.   <code>--mem</code> and <code>--cpu</code> These two options set how much memory and cores you need. Remember that as    a thumb of rule, these two go together. The more cores you use in    parallelization, the more memory you will need since cores are sharing data    over RAM. <code>serc</code> nodes have between 24 and 128 cores, and between  191G and    1024G of RAM memory. Your job should not exceed any of these limits, and    usually not be even close to that maximum. Adding more memory and more cores is    often not a good solution to slow code (check out the piece about the Amdahl's Rule here).  <code>--mail-user</code> and <code>--mail-type</code> This is nice-to-have in your job, especially if you're in a long queue. This    command will send an email to the email under <code>mail-user</code> once the job started    to run, finished to run, or just failed. Usually has nice data on usage that    you can use to set your next job."}, {"location": "computing/#some_tricks_and_common_issues_in_sherlock", "title": "Some tricks and common issues in Sherlock", "text": ""}, {"location": "computing/#tricks", "title": "Tricks", "text": ""}, {"location": "computing/#do_ssh_like_a_pro", "title": "Do <code>ssh</code> like a pro", "text": "<p>Tired of doing <code>ssh &lt;user&gt;@sherlock.stanford.edu</code> and being thrown into whatever login node they want? Want to set up an SSH-tunnel from the get-go with a pre-determined port (v.gr. this is very useful for running Jupyter notebooks in Sherlock nodes)? The SSH config is what you need. The Sherlock documentation has some details on this.  </p>"}, {"location": "computing/#tmux_is_your_best_friend", "title": "<code>tmux</code> is your best friend", "text": "<p>Back in the day, life in the terminal was the common place. Unix has developed very good window managers that allow you to have horizontal and vertical splits, and several windows within a unique <code>ssh</code> connection. More importantly, these managers also allow you to run a process and let it run even if you disconnect from Sherlock. There are two main programs that serve this functions: <code>screen</code> and <code>tmux</code>. The later is better maintained, and is also my favorite \u2764\ufe0f</p> <p>Tmux has a good starter guide, and is extremely configurable using its dotfile: <code>~/.tmux.conf</code>. I know, what the heck are dotfiles? Check out more about them here. </p>"}, {"location": "computing/#building_bash_scripts", "title": "Building bash scripts", "text": "<p>An essential tool of the terminal is to learn how to run basic bash files. These files are very flexible and allow to build command-line interfaces: </p> A simple bash file: <code>hello.sh</code>A bash file with commands <pre><code>#!/bin/bash\necho \"Hello world!\"\n</code></pre> <pre><code>#!/bin/bash\n# Print whatever argument we put on the command\n$ARG=$1\necho \"{$ARG}\"\n</code></pre> <p>Bash is incredibly flexible and powerful. These scripts are particularly useful when creating batch jobs or just simplifying how we run code. More details on how to write and run these are here.</p>"}, {"location": "computing/#faqs", "title": "FAQs", "text": ""}, {"location": "computing/#my_windows_machine_is_not_compatible_with_x_y_or_z", "title": "My Windows machine is not compatible with X, Y or Z", "text": "<p>Truly unfortunate. You have a 2K one-time grant as a SDSS student. Change computers, many good Linux Machines are also available with better specs than a MacBook Pro at a similar price range. If you want to stay with Windows, update to the Windows 10/11 edition and install the Ubuntu terminal. </p>"}, {"location": "computing/#i_dont_have_access_to_oak_what_should_i_do", "title": "I don't have access to <code>$OAK</code>, what should I do?", "text": "Checkout the SERC OAK documentation"}, {"location": "computing/#my_job_is_stuck_in_priority", "title": "My job is stuck in <code>(Priority)</code>", "text": "Life is hard. Just wait"}, {"location": "computing/#i_need_some_help_with_my_code", "title": "I need some help with my code", "text": "Getting help for Sherlock is quite easy. You can either ask for help to people in the lab, or you can also use the <code>#sherlock-users</code> channel on Slack where Kilian Cavalotti or any of the cluster's users are happy to help you with any problem. You can also check many of the great resources put together by the SDSS-CFS"}, {"location": "computing/#a_few_remarks_on_environment_reproducibility", "title": "A few remarks on environment reproducibility", "text": ""}, {"location": "computing/#python", "title": "Python", "text": "<p>Sherlock modules do not include any distribution of Anaconda. If you want to install Conda2 or Mamba, you should install it on your <code>$HOME</code> directory. Nonetheless, these Python distributions are very bloated and do not play well with Sherlock. An alternative to this is to load the <code>python/3.9.0</code> module, and then use <code>vritualenv</code>. </p> Load modules and install virtualenvCreate environmentActivate environment <pre><code>module load python/3.9.0\n# pip will install libraries locally in the $HOME\npip install virtualenv </code></pre> <pre><code>virtualenv -p python $SCRATCH/test_env\n</code></pre> <pre><code>source $SCRATCH/test_env/bin/activate\n\n# Test that I am using my environment python\n# This should print the path to the environment\nwhich python\n\n# Once you check is the right path, you can install some libraries\npip install -r requirements.txt\n</code></pre> <p>Notice that I am creating my environment in the <code>$SCRATCH</code> space, this is due to space limitations in the <code>$HOME</code>. Sherlock only allows 15 Gb in your home directory, thus you should avoid to put environment or big files on it. Once you have reached your quota, Sherlock will stop writing files and this can lead to data corruption. </p> <p>You do not need to use <code>$SCRATCH</code>, you can use either <code>$GROUP_HOME</code> or <code>$GROUP_SCRATCH</code>. Be aware you cannot share environments with people across the group, this because of permissions. A solution to this is always keeping a <code>requirements.txt</code> updated so people can reproduce your Python environment. A more advanced alternative is to use Singularity, a containerization  module created specifically for HPC systems and developed by Stanford. </p>"}, {"location": "computing/#a_note_on_condamamba_on_sherlock", "title": "A note on Conda/Mamba on Sherlock", "text": "<p>By default, <code>conda</code> will install environments in your <code>$HOME</code> directory. Conda does not only install Python libraries, but also many binaries that it needs  to compile package installation, this includes its own version of <code>gcc</code> and  its own <code>C++</code> libraries. Be aware that many of these are not optimized to  Sherlock's hardware, and you should avoid these if you are looking for max performance (i.e. Sherlock has it's own compilation of PyTorch just to play nice with its GPUs). </p> <p>A solution to Conda's storage problem is to change your <code>~/.condarc</code> file and use the following (remember to change your user name in the file below)</p> <code>~/.condarc</code> <pre><code>always_yes: true\nenvs_dirs:\n- /scratch/users/{your user name}/conda_envs\nchannels:\n- conda-forge\n- defaults\nchannel_priority: strict\n</code></pre> <p>More details in how to manage <code>virtualenv</code> environments is here.</p>"}, {"location": "computing/#some_reading_material", "title": "Some reading material", "text": "<ul> <li>Ten Simple Rules for Success with HPC - Imperial College</li> </ul> <ol> <li> <p>This is the environment that contains the path to the executables we want to run. Go ahead and run <code>echo $PATH</code> to see where is the computer looking for executable binaries.\u00a0\u21a9</p> </li> <li> <p>I do recommend to use Mamba over Conda, is not only faster, but also way more reliable and based on the <code>conda-forge</code> channel.\u00a0\u21a9</p> </li> </ol>"}, {"location": "contribute/", "title": "How to contribute to this document?", "text": "<p>This website is hosted on Github pages, created with <code>mkdocs</code>, and is rendered on Github via Github Actions. Any commit done on the <code>main</code> branch will trigger the rendering of this webpage, updating its content on real-time. To make changes to this documents, is important to understand the structure of <code>mkdocs</code>. </p>"}, {"location": "contribute/#documentation_stucture", "title": "Documentation stucture", "text": "<p>The main configuration file for <code>mkdocs</code> is <code>mkdocs.yml</code> always at the root of the repo folder. All details related to general configuration, such as the navigation tree (<code>nav</code>) or the configuration of new plugins. To create a new section, you just need to create a markdown document (the extension is <code>.md</code>) and put it in the right category. For instance, we can create a new section <code>new_doc</code> under General Advice by changing our config file:  </p> mkdocs.yml<pre><code>nav:\n- Home: index.md\n- General Advice: - 'new_doc.md'\n</code></pre> <p>YAML is tricky</p> <p>Remember YAML documents are indented, so keep consistency between each of the levels of the document. Not keeping this consistency can break the correct flow of the pages.</p> <p>Not every change in the documents involve modifying the <code>mkdocs.yml</code>. If you only want to change an existing section, you can just easily just change the document in the <code>docs/</code> folder. You can follow the Markdown documentation  to modify the documents. Additionaly, you can check out some of the additional Python-flavored Markdown plugins from the Material theme docs.</p>"}, {"location": "contribute/#local_rendering", "title": "Local rendering", "text": "<p>Github actions are limited. Despite using a small instance and a lightweight configuration, we only have a limited number of runs available. Thus, to avoid running out of computing hours, you can render the document locally and see live changes on your own browser. You will need <code>python&gt;=3.8</code> and hopefully a virtual environment. </p> <p>Note</p> <p>If you are new to Python. You can try the Miniconda distribution. Is a simple way to get a fast running Python environment without much configuration. Check out the latest version here and use it to create a virtual environment (some tips here). For more advance users, there is also a <code>pyenv</code> alternative.</p> <p>Once you have your prefered Python version you can just:</p> condapyenv <pre><code>conda env create --file environment.yml --force\nconda activate mkdocs_echolab\n</code></pre> <pre><code>pyenv virtualenv 3.8 mkdocs_echolab\npyenv local mkdocs_echolab\npip install -r requirements.txt\n</code></pre> <p>Now, make sure you are in the root of the repo folder and render the document in your <code>localhost</code> by using: <code>mkdocs serve</code>. Notice that any change you make to this documents will be automatically made on the web browser.</p>"}, {"location": "contribute/#password_protected_content", "title": "Password protected content \ud83d\udd12", "text": "<p>This webpage is open to the public. Nonetheless, we can password-protect a page, or a complete section using the many plugins in <code>mkdocs</code>. To add a password-protected document, you just need to change the header of the Markdown file you want to change. Python Markdown allows YAML header configurations to each <code>md</code> file, thus you need to change the key in <code>password</code>. For example: </p> <pre><code>---\nauthor: topcat\ndate: %today\npassword: this_is_a_safe_password\n---\n# A password protected document\n</code></pre>"}, {"location": "contribute/#contribution_etiquette", "title": "Contribution Etiquette", "text": "<p>Before submitting any change, we go by the mantra: \"never commit to the main branch\". Sometimes \ud83d\udca9 happens, and that why Oh Shit Git exists, but in principle you do not commit to the <code>main</code> branch. When working in this document the rules of engagement are defined by branching and PR submission. To do this, simply create a new branch: <code>git checkout &lt;name of new branch&gt;</code> and make all changes there. After pushing all changes to Github, you can create a Pull Request in the repository. </p> <p>Once a PR is discussed and approved by other member of the lab, you can accept the change and merge any changes into the document.</p> <p>Warning</p> <p>Be careful with merge conflicts. Is possible that some merge conflicts might arise if there are changes in conflicting lines. You can always brute force the merge by using the <code>--force</code> flag, or just by forcing it on the PR. This might be the right call 95% of the times, be sure of not be the 5%. </p>"}, {"location": "contribute/#hot-fixes_and_easy_changes", "title": "Hot-fixes and easy changes", "text": "<p>If you are modifying a single file, or just solving small text changes, you can use the Github editor to modify the file. Just go to the <code>.md</code> file you want to modity and click the \u270f\ufe0f: button (you can also press the E key). Once you make your changes, be sure of select the option Create a new branch for this commit and start a pull request. </p> <p>If you consider your change can be directly commited to <code>main</code>, you are wrong. Leave it as a PR and find your nearest ECHOer to have a conversation about your change, and then both merge the PR! We \u2764\ufe0f  teamwork. </p>"}, {"location": "life/", "title": "Life in Palo Alto/Bay Area", "text": "<p>No car, no good life</p>"}, {"location": "palife/", "title": "Palo Alto/Stanford Life", "text": "<p>I didn't pick the PA life, PA life choose me</p>"}, {"location": "postdocs/", "title": "Postdoc Life", "text": "<p>No recs here, yet.</p>"}, {"location": "predocs/", "title": "Predoc Life", "text": "<p>No recs here, yet.</p>"}, {"location": "recs/", "title": "General Recommendations", "text": ""}, {"location": "recs/#coursework", "title": "Coursework", "text": "<p>Something here</p>"}, {"location": "resources/", "title": "Resources", "text": ""}, {"location": "resources/#mental_health", "title": "Mental health", "text": ""}, {"location": "resources/#physical_health", "title": "Physical health", "text": ""}, {"location": "resources/#research", "title": "Research", "text": ""}, {"location": "whois/", "title": "ECHOLab People", "text": "<p>People at FSI + Us</p>"}]}